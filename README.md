Description:
Distributed Web Crawler is designed to crawl and index web pages across multiple
machines efficiently. It utilizes parallel processing to handle large-scale data collection
with improved performance. The system ensures seamless distribution of tasks, enabling
faster and more reliable web crawling. By leveraging multiple machines, it enhances
scalability and fault tolerance in real-time web data extraction.
Tools:
Apache Spark, Scrapy (Python), or custom implementation using distributed task queues
like Celery.
Outcomes:
Demonstrate scalability and fault tolerance in crawling large websites.
